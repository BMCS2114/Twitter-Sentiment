{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Tweet_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h93-dZAbm8SV",
        "outputId": "5986d557-2778-4e50-bbcc-bd62175c97ec",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload() #upload kaggle.json\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-421547c2-49a9-4633-bd94-a8a3370e9039\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-421547c2-49a9-4633-bd94-a8a3370e9039\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqiFOaCgmVXH",
        "outputId": "1fe426bd-f16b-4c86-e8f9-b6e13db6df0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download -d kazanova/sentiment140\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sentiment140.zip to /content\n",
            " 94% 76.0M/80.9M [00:00<00:00, 69.3MB/s]\n",
            "100% 80.9M/80.9M [00:00<00:00, 98.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF7ihqDorUfx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubRZUTflpLL3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "from torchtext import data\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import pickle\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import string\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiIHxyj6r0V4"
      },
      "source": [
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "SEED = 2020\n",
        "EMBED_SIZE = 200\n",
        "MAX_FE = 1000\n",
        "MAX_LEN = 96\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeFblhIZrVR1"
      },
      "source": [
        "## Reading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MK1SvkVybQBk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oICv6iLFbc0b"
      },
      "source": [
        "def preprocess_text(df):\n",
        "    '''Function to preprocess and create corpus'''\n",
        "    new_corpus=[]\n",
        "    vocab={}\n",
        "\n",
        "    for text in tqdm(df[\"tweet\"]):\n",
        "        tweet = text.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
        "        words=[w.translate(tweet) for w in re.sub(r'^@[a-zA-Z]+(_[a-zA-Z]+)?','',text).split() ]\n",
        "        for word in words:\n",
        "          try:  \n",
        "            vocab[word]+=1\n",
        "          except:\n",
        "            vocab[word]=1\n",
        "\n",
        "        new_corpus.append(words)\n",
        "    return new_corpus,vocab\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpSu6XN4d8jL"
      },
      "source": [
        "def tokenizer(corpus,mode='train'):\n",
        "\n",
        "  model_save_name = 'tokenizer.pickle'\n",
        "  path = F\"{model_save_name}\" \n",
        "     \n",
        "\n",
        "  if mode==\"train\":\n",
        "    tokenizer_obj=Tokenizer()\n",
        "    tokenizer_obj.fit_on_texts(corpus)\n",
        "    word_index=tokenizer_obj.word_index\n",
        "\n",
        "    with open(path,'wb') as tok:\n",
        "      pickle.dump(tokenizer_obj,tok,protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  else:\n",
        " \n",
        "    word_index=None\n",
        "    with open('tokenizer.pickle','rb') as tok:\n",
        "       tokenizer= pickle.load(tok)\n",
        "\n",
        "  \n",
        "  sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
        "  tweet_pad=pad_sequences(sequences,\n",
        "                            maxlen=MAX_LEN,\n",
        "                            truncating='post',\n",
        "                            padding='post')\n",
        "    \n",
        "  return tweet_pad,word_index \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlVB3UgorYpO"
      },
      "source": [
        "def load_glove():\n",
        "    \n",
        "    embedding_dict={}\n",
        "    with open('/content/glove.840B.200d.txt') as f:\n",
        "        for line in tqdm(f):\n",
        "            values=line.split()\n",
        "            word=values[0]\n",
        "            try:\n",
        "              vectors=np.asarray(values[1:],'float32')\n",
        "              embedding_dict[word]=vectors\n",
        "            except:\n",
        "              continue\n",
        "    f.close()\n",
        "    \n",
        "    return embedding_dict\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLfgAeEhf5Gy"
      },
      "source": [
        "from collections import defaultdict\n",
        "def prepare_matrix(word_index):\n",
        "    embedding_dict = GloVe(\"twitter.27B\",dim=200)\n",
        "    iiv= defaultdict(int)\n",
        "    oov= defaultdict(int)\n",
        "    num_words = len(word_index)\n",
        "    embedding_matrix = np.zeros((num_words+1, 200))\n",
        "\n",
        "    for word, i in tqdm(word_index.items()):\n",
        "        if i > num_words:\n",
        "            continue\n",
        "\n",
        "        \n",
        "        emb_vec = embedding_dict[word]\n",
        "        if not torch.equal(emb_vec,torch.zeros((200),dtype=torch.float)):\n",
        "          embedding_matrix[i] = emb_vec\n",
        "          iiv[word]=vocab[word]\n",
        "\n",
        "        elif torch.equal(embedding_dict[word.lower()],torch.zeros((200),dtype=torch.float)):\n",
        "          emb_vec = embedding_dict[word.lower()]\n",
        "          embedding_matrix[i] = emb_vec\n",
        "          iiv[word]=vocab[word]\n",
        "\n",
        "        elif torch.equal(embedding_dict[word.title()],torch.zeros((200),dtype=torch.float)):\n",
        "          emb_vec = embedding_dict[word.title()]\n",
        "          embedding_matrix[i] = emb_vec\n",
        "          iiv[word]=vocab[word]\n",
        "\n",
        "        else:\n",
        "          oov[word] = vocab[word]\n",
        "\n",
        "    return embedding_matrix,iiv,oov"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1IQkO2PISxW"
      },
      "source": [
        "#### Model taken from https://www.kaggle.com/artgor/text-modelling-in-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7nlZs2_ItpR"
      },
      "source": [
        "class TweetModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_matrix, lstm_hidden_size=200, gru_hidden_size=128):\n",
        "\n",
        "        super(TweetModel,self).__init__()\n",
        "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = True\n",
        "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
        "\n",
        "        self.gru = nn.GRU(embedding_matrix.shape[1] , gru_hidden_size,num_layers=1, bidirectional=True, batch_first=True)\n",
        "\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "        self.Linear1 = nn.Linear(gru_hidden_size*5,16)\n",
        "        self.Linear2 = nn.Linear(16,1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h_embedding = self.embedding(x)\n",
        "\n",
        "        x, (x_h,x_c) = self.gru(h_embedding)\n",
        "\n",
        "        avg_pool = torch.mean(x, 1)\n",
        "        max_pool, _ = torch.max(x, 1)\n",
        "        concat = torch.cat((avg_pool,x_h,max_pool), 1)\n",
        "        concat = self.Linear1(concat)\n",
        "        out = torch.sigmoid(self.Linear2(concat))\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q4h5SOlI6iC"
      },
      "source": [
        "## Datagenerators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07Y-SC59It7J"
      },
      "source": [
        "class TweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self,train,targets=None,mode=\"train\"):\n",
        "\n",
        "    self.train = train\n",
        "    self.mode=mode\n",
        "    self.targets = targets\n",
        "    \n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.train)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "\n",
        "    x_train_fold =  torch.tensor(self.train[idx],dtype=torch.long).to(DEVICE)\n",
        "    if self.mode=='train':\n",
        "        y_train_fold = torch.tensor(self.targets[idx],dtype=torch.float32).to(DEVICE)\n",
        "        return x_train_fold, y_train_fold\n",
        "    else:\n",
        "        return x_train_fold,0\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxWbwnq8Aq0b"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-0mHzswOzsP"
      },
      "source": [
        "def train_model(train,target,embedding_matrix,nepochs=5,batch_size=64,test_split=0.05):\n",
        "\n",
        "      model_save_name = 'classifier.pt'\n",
        "      path = F\"{model_save_name}\" \n",
        "     \n",
        "      X_train,X_test,y_train,y_test = train_test_split(train,target,test_size = test_split)\n",
        "\n",
        "      train_data = TweetDataset(X_train,y_train)\n",
        "      test_data = TweetDataset(X_test,y_test)\n",
        "\n",
        "      dataloaders= {\"train\":DataLoader(train_data,batch_size=batch_size,shuffle=True),\n",
        "                    \"valid\":DataLoader(test_data,batch_size=batch_size,shuffle=True)}\n",
        "\n",
        "      model = TweetModel(embedding_matrix).to(DEVICE)\n",
        "      loss_fn = torch.nn.BCELoss().cuda()\n",
        "      optimizer = torch.optim.Adam(model.parameters(),lr=1e-3,weight_decay=1e-5)\n",
        "      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, eps=1e-4, verbose=True)\n",
        "\n",
        "      best_loss = {'train':np.inf,\n",
        "                  \"valid\":np.inf}\n",
        "\n",
        "      for epoch in range(nepochs):\n",
        "\n",
        "        epoch_loss = {\"train\":0.00,\n",
        "                      \"valid\":0.00}\n",
        "\n",
        "        for phase in ['train','valid']:\n",
        "\n",
        "              if phase==\"train\":\n",
        "                model = model.train()\n",
        "              else:\n",
        "                model= model.eval()\n",
        "\n",
        "              running_loss = 0.00\n",
        "\n",
        "              for i,(x,y) in enumerate(dataloaders['train']):\n",
        "\n",
        "                  optimizer.zero_grad()\n",
        "                  with torch.set_grad_enabled(phase==\"train\"):\n",
        "\n",
        "                    predict = model(x).squeeze()\n",
        "                    loss = loss_fn(predict,y)\n",
        "\n",
        "\n",
        "\n",
        "                    if phase == \"train\":\n",
        "\n",
        "                      loss.backward()\n",
        "                      optimizer.step()\n",
        "\n",
        "                  running_loss+= loss.item()/len(dataloaders[phase])\n",
        "\n",
        "                  epoch_loss[phase]=running_loss\n",
        "\n",
        "        \n",
        "        print(\"Epoch {}/{}   -   loss: {:5.5f}   -   val_loss: {:5.5f}\".format(epoch+1, nepochs, epoch_loss['train'], epoch_loss['valid']))\n",
        "        scheduler.step(epoch_loss['valid'])\n",
        "        if epoch_loss['valid'] < best_loss['valid']:\n",
        "\n",
        "          print(\"saving model...\")\n",
        "          best_loss = epoch_loss\n",
        "          torch.save(model.state_dict(),path,_use_new_zipfile_serialization=False)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkytsKmrbCB-"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2Ow-Qv80j5o"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7C9WvzULi6P"
      },
      "source": [
        "df = pd.read_csv(\"/content/sentiment140.zip\",encoding=DATASET_ENCODING)\n",
        "df= df.iloc[:,[0,-1]]\n",
        "df.columns = ['sentiment','tweet']\n",
        "df = pd.concat([df.query(\"sentiment==0\").sample(120000),df.query(\"sentiment==4\").sample(120000)])\n",
        "df.sentiment = df.sentiment.map({0:1,4:0})\n",
        "df =  shuffle(df).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIm3USzL8yEW",
        "outputId": "a490a77d-2104-42a8-ed93-cecc1d5f3596",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "df.sentiment.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    120000\n",
              "0    120000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ParZzg2YbKIW",
        "outputId": "2d933191-1edf-4aed-abaa-89c4f883e816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "corpus,vocab = preprocess_text(df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 240000/240000 [00:04<00:00, 59985.97it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WCVYU6jNxcr",
        "outputId": "304c134d-e601-4b72-ec30-b6ca752af328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "231943"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECUb6FkibeF_"
      },
      "source": [
        "tweet_pad,word_index = tokenizer(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mly70YA1bw62"
      },
      "source": [
        "\n",
        "#embedding_dict = load_glove()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtyTAuqGfhl6",
        "outputId": "dbcc2bad-09cf-4132-df4f-ec70e087d0ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "embedding_matrix = prepare_matrix(vocab)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.twitter.27B.zip: 1.52GB [11:40, 2.17MB/s]                            \n",
            "100%|█████████▉| 1192133/1193514 [01:22<00:00, 15087.15it/s]\n",
            "  0%|          | 0/231943 [00:00<?, ?it/s]\u001b[A\n",
            "  1%|▏         | 3442/231943 [00:00<00:06, 34416.83it/s]\u001b[A\n",
            "  3%|▎         | 6816/231943 [00:00<00:06, 34203.19it/s]\u001b[A\n",
            "  4%|▍         | 10186/231943 [00:00<00:06, 34048.14it/s]\u001b[A\n",
            "  6%|▌         | 13750/231943 [00:00<00:06, 34508.21it/s]\u001b[A\n",
            "  8%|▊         | 17500/231943 [00:00<00:06, 35353.23it/s]\u001b[A\n",
            "  9%|▉         | 21151/231943 [00:00<00:05, 35690.91it/s]\u001b[A\n",
            " 11%|█         | 24433/231943 [00:00<00:05, 34775.33it/s]\u001b[A\n",
            " 12%|█▏        | 27636/231943 [00:00<00:06, 29222.55it/s]\u001b[A\n",
            " 13%|█▎        | 30517/231943 [00:00<00:07, 28625.58it/s]\u001b[A\n",
            " 15%|█▍        | 34335/231943 [00:01<00:06, 30948.09it/s]\u001b[A\n",
            " 16%|█▋        | 38062/231943 [00:01<00:05, 32606.83it/s]\u001b[A\n",
            " 18%|█▊        | 41577/231943 [00:01<00:05, 33327.15it/s]\u001b[A\n",
            " 20%|█▉        | 45252/231943 [00:01<00:05, 34284.25it/s]\u001b[A\n",
            " 21%|██        | 48909/231943 [00:01<00:05, 34939.38it/s]\u001b[A\n",
            " 23%|██▎       | 52432/231943 [00:01<00:05, 35025.73it/s]\u001b[A\n",
            " 24%|██▍       | 55954/231943 [00:01<00:05, 34741.78it/s]\u001b[A\n",
            " 26%|██▌       | 59575/231943 [00:01<00:04, 35168.67it/s]\u001b[A\n",
            " 27%|██▋       | 63184/231943 [00:01<00:04, 35438.63it/s]\u001b[A\n",
            " 29%|██▉       | 66736/231943 [00:01<00:04, 33355.65it/s]\u001b[A\n",
            " 30%|███       | 70411/231943 [00:02<00:04, 34305.15it/s]\u001b[A\n",
            " 32%|███▏      | 74123/231943 [00:02<00:04, 35103.15it/s]\u001b[A\n",
            " 34%|███▎      | 77747/231943 [00:02<00:04, 35435.91it/s]\u001b[A\n",
            " 35%|███▌      | 81406/231943 [00:02<00:04, 35773.46it/s]\u001b[A\n",
            " 37%|███▋      | 84997/231943 [00:02<00:04, 31945.62it/s]\u001b[A\n",
            " 38%|███▊      | 88278/231943 [00:02<00:05, 26779.32it/s]\u001b[A\n",
            " 39%|███▉      | 91158/231943 [00:02<00:05, 24620.28it/s]\u001b[A\n",
            " 40%|████      | 93796/231943 [00:02<00:05, 24242.90it/s]\u001b[A\n",
            " 42%|████▏     | 97420/231943 [00:03<00:04, 26915.91it/s]\u001b[A\n",
            " 44%|████▎     | 101012/231943 [00:03<00:04, 29104.33it/s]\u001b[A\n",
            " 45%|████▌     | 104504/231943 [00:03<00:04, 30633.99it/s]\u001b[A\n",
            " 47%|████▋     | 108099/231943 [00:03<00:03, 32053.71it/s]\u001b[A\n",
            " 48%|████▊     | 111733/231943 [00:03<00:03, 33229.15it/s]\u001b[A\n",
            " 50%|████▉     | 115257/231943 [00:03<00:03, 33806.51it/s]\u001b[A\n",
            " 51%|█████     | 118707/231943 [00:03<00:03, 33847.46it/s]\u001b[A\n",
            " 53%|█████▎    | 122217/231943 [00:03<00:03, 34213.46it/s]\u001b[A\n",
            " 54%|█████▍    | 125825/231943 [00:03<00:03, 34751.35it/s]\u001b[A\n",
            " 56%|█████▌    | 129439/231943 [00:03<00:02, 35154.66it/s]\u001b[A\n",
            " 57%|█████▋    | 133064/231943 [00:04<00:02, 35474.62it/s]\u001b[A\n",
            " 59%|█████▉    | 136735/231943 [00:04<00:02, 35834.70it/s]\u001b[A\n",
            " 61%|██████    | 140330/231943 [00:04<00:02, 35531.19it/s]\u001b[A\n",
            " 62%|██████▏   | 143943/231943 [00:04<00:02, 35707.23it/s]\u001b[A\n",
            " 64%|██████▎   | 147537/231943 [00:04<00:02, 35775.46it/s]\u001b[A\n",
            " 65%|██████▌   | 151119/231943 [00:04<00:02, 35520.61it/s]\u001b[A\n",
            " 67%|██████▋   | 154675/231943 [00:04<00:02, 34596.20it/s]\u001b[A\n",
            " 68%|██████▊   | 158143/231943 [00:04<00:02, 33179.60it/s]\u001b[A\n",
            " 70%|██████▉   | 161479/231943 [00:04<00:02, 28461.00it/s]\u001b[A\n",
            " 71%|███████   | 164456/231943 [00:05<00:02, 25494.95it/s]\u001b[A\n",
            " 72%|███████▏  | 167155/231943 [00:05<00:02, 23794.49it/s]\u001b[A\n",
            " 73%|███████▎  | 169662/231943 [00:05<00:02, 22455.74it/s]\u001b[A\n",
            " 74%|███████▍  | 172013/231943 [00:05<00:02, 21806.16it/s]\u001b[A\n",
            " 75%|███████▌  | 174271/231943 [00:05<00:02, 21520.85it/s]\u001b[A\n",
            " 76%|███████▌  | 176479/231943 [00:05<00:02, 20784.21it/s]\u001b[A\n",
            " 77%|███████▋  | 178832/231943 [00:05<00:02, 21537.70it/s]\u001b[A\n",
            " 78%|███████▊  | 181024/231943 [00:05<00:02, 21574.98it/s]\u001b[A\n",
            " 79%|███████▉  | 183608/231943 [00:05<00:02, 22698.80it/s]\u001b[A\n",
            " 81%|████████  | 187340/231943 [00:06<00:01, 25721.41it/s]\u001b[A\n",
            " 82%|████████▏ | 190945/231943 [00:06<00:01, 28138.68it/s]\u001b[A\n",
            " 84%|████████▍ | 194560/231943 [00:06<00:01, 30141.25it/s]\u001b[A\n",
            " 85%|████████▌ | 198191/231943 [00:06<00:01, 31758.43it/s]\u001b[A\n",
            " 87%|████████▋ | 201740/231943 [00:06<00:00, 32791.66it/s]\u001b[A\n",
            " 89%|████████▊ | 205289/231943 [00:06<00:00, 33555.77it/s]\u001b[A\n",
            " 90%|█████████ | 208758/231943 [00:06<00:00, 33886.86it/s]\u001b[A\n",
            " 92%|█████████▏| 212473/231943 [00:06<00:00, 34801.91it/s]\u001b[A\n",
            " 93%|█████████▎| 215999/231943 [00:06<00:00, 34815.17it/s]\u001b[A\n",
            " 95%|█████████▍| 219595/231943 [00:06<00:00, 35150.48it/s]\u001b[A\n",
            " 96%|█████████▌| 223220/231943 [00:07<00:00, 35472.09it/s]\u001b[A\n",
            " 98%|█████████▊| 226808/231943 [00:07<00:00, 35590.82it/s]\u001b[A\n",
            "100%|██████████| 231943/231943 [00:07<00:00, 31686.00it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoycP4TNFems",
        "outputId": "2e3d6121-d23e-468d-d0c7-4d5222ce223f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>tweet</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>190784</th>\n",
              "      <td>0</td>\n",
              "      <td>@kyteacher for printing/viewing   2/2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>79390</th>\n",
              "      <td>0</td>\n",
              "      <td>@aNorthernSoul That's a nice pic. Thx for shar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152020</th>\n",
              "      <td>0</td>\n",
              "      <td>is reading 7th Period is a Secret.  http://plu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20810</th>\n",
              "      <td>0</td>\n",
              "      <td>twiter twitter guyys....we r goiin crazy in bu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35023</th>\n",
              "      <td>0</td>\n",
              "      <td>@bethanyvarrone aj holyfield  yeahhh hes sexyy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>219591</th>\n",
              "      <td>0</td>\n",
              "      <td>@Emsy I'm sure they're ALL still asleep!  You'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>58051</th>\n",
              "      <td>0</td>\n",
              "      <td>Jst had an economics exam that went well and n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>187498</th>\n",
              "      <td>1</td>\n",
              "      <td>@manny138 i miss not seeing you at 8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91064</th>\n",
              "      <td>1</td>\n",
              "      <td>@courtcothren we miss you... wish we could hea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120092</th>\n",
              "      <td>0</td>\n",
              "      <td>@mileycyrus: hey miley!! I just wanted to let ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        sentiment                                              tweet\n",
              "190784          0              @kyteacher for printing/viewing   2/2\n",
              "79390           0  @aNorthernSoul That's a nice pic. Thx for shar...\n",
              "152020          0  is reading 7th Period is a Secret.  http://plu...\n",
              "20810           0  twiter twitter guyys....we r goiin crazy in bu...\n",
              "35023           0     @bethanyvarrone aj holyfield  yeahhh hes sexyy\n",
              "219591          0  @Emsy I'm sure they're ALL still asleep!  You'...\n",
              "58051           0  Jst had an economics exam that went well and n...\n",
              "187498          1              @manny138 i miss not seeing you at 8 \n",
              "91064           1  @courtcothren we miss you... wish we could hea...\n",
              "120092          0  @mileycyrus: hey miley!! I just wanted to let ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7zj40PTehJ7"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Viq_xkejKX"
      },
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(tweet_pad,df.sentiment.values,test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBojBPN7e12B",
        "outputId": "0d66e1ca-39f1-40dc-d163-c093d1a16d49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train_model(X_train,y_train,embedding_matrix[0],nepochs=40)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 1192133/1193514 [01:40<00:00, 15087.15it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40   -   loss: 0.46641   -   val_loss: 6.96229\n",
            "saving model...\n",
            "Epoch 2/40   -   loss: 0.39392   -   val_loss: 5.88400\n",
            "saving model...\n",
            "Epoch 3/40   -   loss: 0.33149   -   val_loss: 4.25448\n",
            "saving model...\n",
            "Epoch 4/40   -   loss: 0.24637   -   val_loss: 2.83733\n",
            "saving model...\n",
            "Epoch 5/40   -   loss: 0.18314   -   val_loss: 2.15942\n",
            "saving model...\n",
            "Epoch 6/40   -   loss: 0.15002   -   val_loss: 1.62253\n",
            "saving model...\n",
            "Epoch 7/40   -   loss: 0.12933   -   val_loss: 1.46513\n",
            "saving model...\n",
            "Epoch 8/40   -   loss: 0.11456   -   val_loss: 1.38749\n",
            "saving model...\n",
            "Epoch 9/40   -   loss: 0.10197   -   val_loss: 1.12508\n",
            "saving model...\n",
            "Epoch 10/40   -   loss: 0.09648   -   val_loss: 0.99576\n",
            "saving model...\n",
            "Epoch 11/40   -   loss: 0.09031   -   val_loss: 0.93475\n",
            "saving model...\n",
            "Epoch 12/40   -   loss: 0.08470   -   val_loss: 1.00877\n",
            "Epoch 13/40   -   loss: 0.08046   -   val_loss: 0.83449\n",
            "saving model...\n",
            "Epoch 14/40   -   loss: 0.07608   -   val_loss: 0.77518\n",
            "saving model...\n",
            "Epoch 15/40   -   loss: 0.07383   -   val_loss: 0.74925\n",
            "saving model...\n",
            "Epoch 16/40   -   loss: 0.07467   -   val_loss: 0.78605\n",
            "Epoch 17/40   -   loss: 0.07083   -   val_loss: 0.76773\n",
            "Epoch 18/40   -   loss: 0.06722   -   val_loss: 0.73148\n",
            "saving model...\n",
            "Epoch 19/40   -   loss: 0.06802   -   val_loss: 0.82122\n",
            "Epoch 20/40   -   loss: 0.06603   -   val_loss: 0.69066\n",
            "saving model...\n",
            "Epoch 21/40   -   loss: 0.06530   -   val_loss: 0.66280\n",
            "saving model...\n",
            "Epoch 22/40   -   loss: 0.06410   -   val_loss: 0.64990\n",
            "saving model...\n",
            "Epoch 23/40   -   loss: 0.06293   -   val_loss: 0.63278\n",
            "saving model...\n",
            "Epoch 24/40   -   loss: 0.06389   -   val_loss: 0.59376\n",
            "saving model...\n",
            "Epoch 25/40   -   loss: 0.05973   -   val_loss: 0.69424\n",
            "Epoch 26/40   -   loss: 0.06146   -   val_loss: 0.63747\n",
            "Epoch 27/40   -   loss: 0.06019   -   val_loss: 0.56720\n",
            "saving model...\n",
            "Epoch 28/40   -   loss: 0.05634   -   val_loss: 0.55331\n",
            "saving model...\n",
            "Epoch 29/40   -   loss: 0.05930   -   val_loss: 0.55374\n",
            "Epoch 30/40   -   loss: 0.05767   -   val_loss: 0.55313\n",
            "saving model...\n",
            "Epoch 31/40   -   loss: 0.05620   -   val_loss: 0.58782\n",
            "Epoch 32/40   -   loss: 0.05690   -   val_loss: 0.59930\n",
            "Epoch 33/40   -   loss: 0.05634   -   val_loss: 0.62117\n",
            "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
            "Epoch 34/40   -   loss: 0.02429   -   val_loss: 0.16780\n",
            "saving model...\n",
            "Epoch 35/40   -   loss: 0.00872   -   val_loss: 0.10422\n",
            "saving model...\n",
            "Epoch 36/40   -   loss: 0.00774   -   val_loss: 0.09094\n",
            "saving model...\n",
            "Epoch 37/40   -   loss: 0.01206   -   val_loss: 0.09657\n",
            "Epoch 38/40   -   loss: 0.01284   -   val_loss: 0.09069\n",
            "saving model...\n",
            "Epoch 39/40   -   loss: 0.01017   -   val_loss: 0.08435\n",
            "saving model...\n",
            "Epoch 40/40   -   loss: 0.00837   -   val_loss: 0.08176\n",
            "saving model...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj3DIKRagzRl"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCM2gl8Xe8cx"
      },
      "source": [
        "dataloader = DataLoader(TweetDataset(X_test,mode=\"test\"),batch_size=32,shuffle=False)\n",
        "model = TweetModel(embedding_matrix=embedding_matrix[0]).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"classifier.pt\"))\n",
        "model.eval()\n",
        "batch_preds = []\n",
        "predictions=[]\n",
        "for x,y in dataloader:\n",
        "  x= x.to(DEVICE)\n",
        "  with torch.no_grad():\n",
        "    batch_preds = model(x)\n",
        "\n",
        "  predictions.append(batch_preds)\n",
        "\n",
        "predictions = torch.cat(predictions, dim=0).cpu().numpy()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkaL4X75gwst",
        "outputId": "c728559b-3211-4eba-8a9b-6f1cd50440cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(roc_auc_score(np.round(predictions).squeeze(),y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7679167383336202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdVq5Qxhq7sc",
        "outputId": "549cbf78-a09c-44d0-96dd-a74946be4ee6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.download(\"tokenizer.pickle\")\n",
        "files.download(\"classifier.pt\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5a7f8839-18ad-4004-ac0c-fa7925b1ee37\", \"tokenizer.pickle\", 9139512)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_99d630e8-3c48-449a-bbfd-ef468dc337f9\", \"classifier.pt\", 186611946)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}