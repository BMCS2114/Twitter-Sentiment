{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tweet_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "h93-dZAbm8SV",
        "outputId": "af1f22fd-55b4-470d-ea88-dfddd8a8dba9",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload() #upload kaggle.json\n",
        "\n",
        "!pip install -q kaggle\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!ls ~/.kaggle\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d2957d8f-0e9b-483a-8d43-2a8e14c672e8\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d2957d8f-0e9b-483a-8d43-2a8e14c672e8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "kaggle.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_AYSw5xoXZG"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqiFOaCgmVXH",
        "outputId": "962d67a1-5da1-4ad9-9671-4efdcba03e6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "!kaggle datasets download -d kazanova/sentiment140\n",
        "#!kaggle datasets download -d takuok/glove840b300dtxt\n",
        "#!unzip /content/glove840b300dtxt.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading sentiment140.zip to /content\n",
            " 94% 76.0M/80.9M [00:01<00:00, 72.5MB/s]\n",
            "100% 80.9M/80.9M [00:01<00:00, 64.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mF7ihqDorUfx"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubRZUTflpLL3",
        "outputId": "98e681e2-609d-4622-a364-c997641a76e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "from torchtext import data\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import pickle\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "nltk.download('all')\n",
        "nltk.download('punkt')\n",
        "stop=stopwords.words(\"english\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiIHxyj6r0V4"
      },
      "source": [
        "DATASET_ENCODING = \"ISO-8859-1\"\n",
        "SEED = 2020\n",
        "EMBED_SIZE = 300\n",
        "MAX_FE = 1000\n",
        "MAX_LEN = 80\n",
        "torch.manual_seed(SEED)\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeFblhIZrVR1"
      },
      "source": [
        "## Reading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oICv6iLFbc0b"
      },
      "source": [
        "def preprocess_text(df):\n",
        "    '''Function to preprocess and create corpus'''\n",
        "    new_corpus=[]\n",
        "    vocab={}\n",
        "    for text in tqdm(df[\"tweet\"]):\n",
        "        words=[w for w in word_tokenize(re.sub(r'^@[a-zA-Z]+','',text)) ]\n",
        "        for word in words:\n",
        "          try:  \n",
        "            vocab[word]+=1\n",
        "          except:\n",
        "            vocab[word]=1\n",
        "\n",
        "        new_corpus.append(words)\n",
        "    return new_corpus,vocab\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpSu6XN4d8jL"
      },
      "source": [
        "def tokenizer(corpus,mode='train'):\n",
        "\n",
        "  model_save_name = 'tokenizer.pickle'\n",
        "  path = F\"{model_save_name}\" \n",
        "     \n",
        "\n",
        "  if mode==\"train\":\n",
        "    tokenizer_obj=Tokenizer()\n",
        "    tokenizer_obj.fit_on_texts(corpus)\n",
        "    word_index=tokenizer_obj.word_index\n",
        "\n",
        "    with open(path,'wb') as tok:\n",
        "      pickle.dump(tokenizer_obj,tok,protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "  else:\n",
        " \n",
        "    word_index=None\n",
        "    with open('tokenizer.pickle','rb') as tok:\n",
        "       tokenizer= pickle.load(tok)\n",
        "\n",
        "  \n",
        "  sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
        "  tweet_pad=pad_sequences(sequences,\n",
        "                            maxlen=MAX_LEN,\n",
        "                            truncating='post',\n",
        "                            padding='post')\n",
        "    \n",
        "  return tweet_pad,word_index \n",
        "    "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlVB3UgorYpO"
      },
      "source": [
        "def load_glove():\n",
        "    \n",
        "    embedding_dict={}\n",
        "    with open('/content/glove.840B.300d.txt') as f:\n",
        "        for line in tqdm(f):\n",
        "            values=line.split()\n",
        "            word=values[0]\n",
        "            try:\n",
        "              vectors=np.asarray(values[1:],'float32')\n",
        "              embedding_dict[word]=vectors\n",
        "            except:\n",
        "              continue\n",
        "    f.close()\n",
        "    \n",
        "    return embedding_dict\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLfgAeEhf5Gy"
      },
      "source": [
        "from collections import defaultdict\n",
        "def prepare_matrix(word_index):\n",
        "    embedding_dict = GloVe(\"twitter.27B\",dim=200)\n",
        "    iiv= defaultdict(int)\n",
        "    oov= defaultdict(int)\n",
        "    num_words = len(word_index)\n",
        "    embedding_matrix = np.zeros((num_words+1, 200))\n",
        "\n",
        "    for word, i in tqdm(word_index.items()):\n",
        "        if i > num_words:\n",
        "            continue\n",
        "\n",
        "        \n",
        "        emb_vec = embedding_dict[word]\n",
        "        if not torch.equal(emb_vec,torch.zeros((200),dtype=torch.float)):\n",
        "          embedding_matrix[i] = emb_vec\n",
        "          iiv[word]=vocab[word]\n",
        "\n",
        "        elif torch.equal(embedding_dict[word.lower()],torch.zeros((200),dtype=torch.float)):\n",
        "          emb_vec = embedding_dict[word.lower()]\n",
        "          embedding_matrix[i] = emb_vec\n",
        "          iiv[word]=vocab[word]\n",
        "\n",
        "        elif torch.equal(embedding_dict[word.title()],torch.zeros((200),dtype=torch.float)):\n",
        "          emb_vec = embedding_dict[word.title()]\n",
        "          embedding_matrix[i] = emb_vec\n",
        "          iiv[word]=vocab[word]\n",
        "\n",
        "        else:\n",
        "          oov[word] = vocab[word]\n",
        "\n",
        "    return embedding_matrix,iiv,oov"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1IQkO2PISxW"
      },
      "source": [
        "#### Model taken from https://www.kaggle.com/artgor/text-modelling-in-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7nlZs2_ItpR"
      },
      "source": [
        "class TweetModel(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_matrix, lstm_hidden_size=256, gru_hidden_size=64):\n",
        "\n",
        "        super(TweetModel,self).__init__()\n",
        "        self.embedding = nn.Embedding(*embedding_matrix.shape)\n",
        "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
        "        self.embedding.weight.requires_grad = False\n",
        "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
        "\n",
        "        self.lstm = nn.LSTM(embedding_matrix.shape[1], lstm_hidden_size, bidirectional=True, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(lstm_hidden_size*2 , gru_hidden_size, bidirectional=True, batch_first=True)\n",
        "        self.dropout2 = nn.Dropout(0.25)\n",
        "        self.Linear1 = nn.Linear(gru_hidden_size*4,64)\n",
        "        self.Linear2 = nn.Linear(64,1)\n",
        "        \n",
        "    def apply_spatial_dropout(self, h_embedding):\n",
        "        h_embedding = h_embedding.transpose(1, 2).unsqueeze(2)\n",
        "        h_embedding = self.embedding_dropout(h_embedding).squeeze(2).transpose(1, 2)\n",
        "        return h_embedding\n",
        "    \n",
        "    def flatten_parameters(self):\n",
        "        self.lstm.flatten_parameters()\n",
        "        self.lstm2.flatten_parameters()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        h_embedding = self.embedding(x)\n",
        "        h_embedding = self.apply_spatial_dropout(h_embedding)\n",
        "\n",
        "        h_lstm, _ = self.lstm(h_embedding)\n",
        "        h_lstm, _ = self.lstm2(h_lstm)\n",
        "\n",
        "        avg_pool = torch.mean(h_lstm, 1)\n",
        "        max_pool, _ = torch.max(h_lstm, 1)\n",
        "        concat = self.Linear1(torch.cat((avg_pool, max_pool), 1))\n",
        "        #out = self.dropout2(concat)\n",
        "        out = torch.sigmoid(self.Linear2(concat))\n",
        "        return out"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q4h5SOlI6iC"
      },
      "source": [
        "## Datagenerators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07Y-SC59It7J"
      },
      "source": [
        "class TweetDataset(Dataset):\n",
        "\n",
        "  def __init__(self,train,targets=None,mode=\"train\"):\n",
        "\n",
        "    self.train = train\n",
        "    self.mode=mode\n",
        "    self.targets = targets\n",
        "    \n",
        "  def __len__(self):\n",
        "\n",
        "    return len(self.train)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "\n",
        "    x_train_fold =  torch.tensor(self.train[idx],dtype=torch.long).to(DEVICE)\n",
        "    if self.mode=='train':\n",
        "        y_train_fold = torch.tensor(self.targets[idx],dtype=torch.float32).to(DEVICE)\n",
        "        return x_train_fold, y_train_fold\n",
        "    else:\n",
        "        return x_train_fold,0\n",
        "\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxWbwnq8Aq0b"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-0mHzswOzsP"
      },
      "source": [
        "def train_model(train,target,embedding_matrix,nepochs=5,batch_size=32,test_split=0.15):\n",
        "\n",
        "      model_save_name = 'classifier.pt'\n",
        "      path = F\"{model_save_name}\" \n",
        "     \n",
        "      X_train,X_test,y_train,y_test = train_test_split(train,target,test_size = test_split)\n",
        "\n",
        "      train_data = TweetDataset(X_train,y_train)\n",
        "      test_data = TweetDataset(X_test,y_test)\n",
        "\n",
        "      dataloaders= {\"train\":DataLoader(train_data,batch_size=batch_size,shuffle=True),\n",
        "                    \"valid\":DataLoader(test_data,batch_size=batch_size,shuffle=False)}\n",
        "\n",
        "      model = TweetModel(embedding_matrix).to(DEVICE)\n",
        "      loss_fn = torch.nn.BCELoss().cuda()\n",
        "      optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
        "      scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, eps=1e-4, verbose=True)\n",
        "\n",
        "      best_loss = {'train':np.inf,\n",
        "                  \"valid\":np.inf}\n",
        "\n",
        "      for epoch in range(nepochs):\n",
        "\n",
        "        epoch_loss = {\"train\":0.00,\n",
        "                      \"valid\":0.00}\n",
        "\n",
        "        for phase in ['train','valid']:\n",
        "\n",
        "              if phase==\"train\":\n",
        "                model = model.train()\n",
        "              else:\n",
        "                model= model.eval()\n",
        "\n",
        "              running_loss = 0.00\n",
        "\n",
        "              for i,(x,y) in enumerate(dataloaders['train']):\n",
        "\n",
        "                  optimizer.zero_grad()\n",
        "                  with torch.set_grad_enabled(phase==\"train\"):\n",
        "\n",
        "                    predict = model(x).squeeze()\n",
        "                    loss = loss_fn(predict,y)\n",
        "\n",
        "\n",
        "\n",
        "                    if phase == \"train\":\n",
        "\n",
        "                      loss.backward()\n",
        "                      optimizer.step()\n",
        "\n",
        "                  running_loss+= loss.item()/len(dataloaders[phase])\n",
        "\n",
        "                  epoch_loss[phase]=running_loss\n",
        "\n",
        "        \n",
        "        print(\"Epoch {}/{}   -   loss: {:5.5f}   -   val_loss: {:5.5f}\".format(epoch+1, nepochs, epoch_loss['train'], epoch_loss['valid']))\n",
        "        scheduler.step(epoch_loss['valid'])\n",
        "        if epoch_loss['valid'] < best_loss['valid']:\n",
        "\n",
        "          print(\"saving model...\")\n",
        "          best_loss = epoch_loss\n",
        "          torch.save(model.state_dict(),path,_use_new_zipfile_serialization=False)\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkytsKmrbCB-"
      },
      "source": [
        "## Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2Ow-Qv80j5o"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "import re"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7C9WvzULi6P"
      },
      "source": [
        "df = pd.read_csv(\"/content/sentiment140.zip\",encoding=DATASET_ENCODING)\n",
        "df= df.iloc[:,[0,-1]]\n",
        "df.columns = ['sentiment','tweet']\n",
        "df = pd.concat([df.query(\"sentiment==0\").sample(110000),df.query(\"sentiment==4\").sample(110000)])\n",
        "df.sentiment = df.sentiment.map({0:0,4:1})\n",
        "df =  shuffle(df).reset_index(drop=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1xEbU0CaZJc"
      },
      "source": [
        ""
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ParZzg2YbKIW",
        "outputId": "9bd68e54-5d0c-456a-eb7e-063327dbe7f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "corpus,vocab = preprocess_text(df)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 220000/220000 [00:51<00:00, 4239.29it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WCVYU6jNxcr",
        "outputId": "2dcd6e38-ade9-4ce5-c917-e990dda3e23c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "163806"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECUb6FkibeF_"
      },
      "source": [
        "tweet_pad,word_index = tokenizer(corpus)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mly70YA1bw62"
      },
      "source": [
        "\n",
        "#embedding_dict = load_glove()\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtyTAuqGfhl6",
        "outputId": "4b55784b-5931-47dc-b673-2386f3fa0272",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "embedding_matrix = prepare_matrix(vocab)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.twitter.27B.zip: 1.52GB [11:44, 2.16MB/s]                           \n",
            "100%|█████████▉| 1192560/1193514 [01:56<00:00, 10023.62it/s]\n",
            "  0%|          | 0/163806 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 96/163806 [00:00<02:50, 959.92it/s]\u001b[A\n",
            "  1%|▏         | 2402/163806 [00:00<01:59, 1347.28it/s]\u001b[A\n",
            "  3%|▎         | 4284/163806 [00:00<01:25, 1867.39it/s]\u001b[A\n",
            "  4%|▍         | 6536/163806 [00:00<01:01, 2576.11it/s]\u001b[A\n",
            "  5%|▌         | 8735/163806 [00:00<00:44, 3504.19it/s]\u001b[A\n",
            "  7%|▋         | 11004/163806 [00:00<00:32, 4695.15it/s]\u001b[A\n",
            "  8%|▊         | 13002/163806 [00:00<00:24, 6093.60it/s]\u001b[A\n",
            "  9%|▉         | 15003/163806 [00:00<00:19, 7699.97it/s]\u001b[A\n",
            " 11%|█         | 17270/163806 [00:00<00:15, 9602.09it/s]\u001b[A\n",
            " 12%|█▏        | 19511/163806 [00:01<00:12, 11588.28it/s]\u001b[A\n",
            " 13%|█▎        | 21743/163806 [00:01<00:10, 13541.53it/s]\u001b[A\n",
            " 15%|█▍        | 23875/163806 [00:01<00:09, 15201.02it/s]\u001b[A\n",
            " 16%|█▌        | 26006/163806 [00:01<00:08, 16101.54it/s]\u001b[A\n",
            " 17%|█▋        | 28214/163806 [00:01<00:07, 17524.09it/s]\u001b[A\n",
            " 19%|█▊        | 30431/163806 [00:01<00:07, 18698.08it/s]\u001b[A\n",
            " 20%|█▉        | 32566/163806 [00:01<00:06, 19002.10it/s]\u001b[A\n",
            " 21%|██        | 34653/163806 [00:01<00:06, 18546.57it/s]\u001b[A\n",
            " 22%|██▏       | 36641/163806 [00:01<00:06, 18830.86it/s]\u001b[A\n",
            " 24%|██▎       | 38619/163806 [00:01<00:06, 19020.63it/s]\u001b[A\n",
            " 25%|██▍       | 40635/163806 [00:02<00:06, 19347.39it/s]\u001b[A\n",
            " 26%|██▌       | 42618/163806 [00:02<00:06, 19193.42it/s]\u001b[A\n",
            " 27%|██▋       | 44590/163806 [00:02<00:06, 19347.38it/s]\u001b[A\n",
            " 28%|██▊       | 46549/163806 [00:02<00:06, 18555.83it/s]\u001b[A\n",
            " 30%|██▉       | 48516/163806 [00:02<00:06, 18875.22it/s]\u001b[A\n",
            " 31%|███       | 50484/163806 [00:02<00:05, 19108.82it/s]\u001b[A\n",
            " 32%|███▏      | 52456/163806 [00:02<00:05, 19287.72it/s]\u001b[A\n",
            " 33%|███▎      | 54525/163806 [00:02<00:05, 19687.02it/s]\u001b[A\n",
            " 34%|███▍      | 56503/163806 [00:02<00:05, 19420.46it/s]\u001b[A\n",
            " 36%|███▌      | 58453/163806 [00:02<00:05, 19436.40it/s]\u001b[A\n",
            " 37%|███▋      | 60481/163806 [00:03<00:05, 19681.19it/s]\u001b[A\n",
            " 38%|███▊      | 62504/163806 [00:03<00:05, 19842.62it/s]\u001b[A\n",
            " 39%|███▉      | 64492/163806 [00:03<00:05, 19826.63it/s]\u001b[A\n",
            " 41%|████      | 66477/163806 [00:03<00:05, 19043.75it/s]\u001b[A\n",
            " 42%|████▏     | 68390/163806 [00:03<00:05, 18900.28it/s]\u001b[A\n",
            " 43%|████▎     | 70355/163806 [00:03<00:04, 19118.83it/s]\u001b[A\n",
            " 44%|████▍     | 72341/163806 [00:03<00:04, 19333.71it/s]\u001b[A\n",
            " 45%|████▌     | 74287/163806 [00:03<00:04, 19368.42it/s]\u001b[A\n",
            " 47%|████▋     | 76292/163806 [00:03<00:04, 19567.13it/s]\u001b[A\n",
            " 48%|████▊     | 78252/163806 [00:03<00:04, 19322.20it/s]\u001b[A\n",
            " 49%|████▉     | 80199/163806 [00:04<00:04, 19364.23it/s]\u001b[A\n",
            " 50%|█████     | 82150/163806 [00:04<00:04, 19406.33it/s]\u001b[A\n",
            " 51%|█████▏    | 84149/163806 [00:04<00:04, 19576.08it/s]\u001b[A\n",
            " 53%|█████▎    | 86108/163806 [00:04<00:04, 18762.02it/s]\u001b[A\n",
            " 54%|█████▎    | 87993/163806 [00:04<00:04, 18504.79it/s]\u001b[A\n",
            " 55%|█████▍    | 89982/163806 [00:04<00:03, 18898.69it/s]\u001b[A\n",
            " 56%|█████▌    | 92002/163806 [00:04<00:03, 19269.69it/s]\u001b[A\n",
            " 57%|█████▋    | 93956/163806 [00:04<00:03, 19347.42it/s]\u001b[A\n",
            " 59%|█████▊    | 95993/163806 [00:04<00:03, 19641.31it/s]\u001b[A\n",
            " 60%|█████▉    | 98070/163806 [00:05<00:03, 19962.64it/s]\u001b[A\n",
            " 61%|██████    | 100071/163806 [00:05<00:03, 19339.50it/s]\u001b[A\n",
            " 62%|██████▏   | 102013/163806 [00:05<00:03, 18960.27it/s]\u001b[A\n",
            " 64%|██████▎   | 104026/163806 [00:05<00:03, 19295.15it/s]\u001b[A\n",
            " 65%|██████▍   | 105962/163806 [00:05<00:03, 18683.53it/s]\u001b[A\n",
            " 66%|██████▌   | 107839/163806 [00:05<00:03, 18473.15it/s]\u001b[A\n",
            " 67%|██████▋   | 109838/163806 [00:05<00:02, 18899.71it/s]\u001b[A\n",
            " 68%|██████▊   | 111803/163806 [00:05<00:02, 19118.01it/s]\u001b[A\n",
            " 69%|██████▉   | 113784/163806 [00:05<00:02, 19320.32it/s]\u001b[A\n",
            " 71%|███████   | 115721/163806 [00:05<00:02, 19086.71it/s]\u001b[A\n",
            " 72%|███████▏  | 117654/163806 [00:06<00:02, 19157.48it/s]\u001b[A\n",
            " 73%|███████▎  | 119580/163806 [00:06<00:02, 19186.72it/s]\u001b[A\n",
            " 74%|███████▍  | 121562/163806 [00:06<00:02, 19371.06it/s]\u001b[A\n",
            " 75%|███████▌  | 123522/163806 [00:06<00:02, 19438.61it/s]\u001b[A\n",
            " 77%|███████▋  | 125468/163806 [00:06<00:02, 18613.73it/s]\u001b[A\n",
            " 78%|███████▊  | 127465/163806 [00:06<00:01, 18999.85it/s]\u001b[A\n",
            "100%|█████████▉| 1192560/1193514 [02:10<00:00, 10023.62it/s]\n",
            " 80%|████████  | 131418/163806 [00:06<00:01, 19040.13it/s]\u001b[A\n",
            " 81%|████████▏ | 133474/163806 [00:06<00:01, 19471.29it/s]\u001b[A\n",
            " 83%|████████▎ | 135454/163806 [00:06<00:01, 19567.50it/s]\u001b[A\n",
            " 84%|████████▍ | 137478/163806 [00:07<00:01, 19762.61it/s]\u001b[A\n",
            " 85%|████████▌ | 139477/163806 [00:07<00:01, 19828.39it/s]\u001b[A\n",
            " 86%|████████▋ | 141469/163806 [00:07<00:01, 19855.10it/s]\u001b[A\n",
            " 88%|████████▊ | 143468/163806 [00:07<00:01, 19893.99it/s]\u001b[A\n",
            " 89%|████████▉ | 145459/163806 [00:07<00:00, 19048.43it/s]\u001b[A\n",
            " 90%|████████▉ | 147373/163806 [00:07<00:00, 19069.15it/s]\u001b[A\n",
            " 91%|█████████ | 149362/163806 [00:07<00:00, 19305.63it/s]\u001b[A\n",
            " 92%|█████████▏| 151298/163806 [00:07<00:00, 19241.92it/s]\u001b[A\n",
            " 94%|█████████▎| 153226/163806 [00:07<00:00, 19128.94it/s]\u001b[A\n",
            " 95%|█████████▍| 155142/163806 [00:07<00:00, 18702.16it/s]\u001b[A\n",
            " 96%|█████████▌| 157018/163806 [00:08<00:00, 18719.13it/s]\u001b[A\n",
            " 97%|█████████▋| 158975/163806 [00:08<00:00, 18966.28it/s]\u001b[A\n",
            " 98%|█████████▊| 160959/163806 [00:08<00:00, 19217.85it/s]\u001b[A\n",
            "100%|██████████| 163806/163806 [00:08<00:00, 19351.35it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS7Pn_iOedeW",
        "outputId": "3cc0496d-dd30-45b3-dd03-2d4f20cb7f15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import operator\n",
        "oov = sorted(embedding_matrix[2].items(),key=lambda x : x[1], reverse=True)\n",
        "len(embedding_matrix[1])/len(vocab)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN_z1w1HWNi_"
      },
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7zj40PTehJ7"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-Viq_xkejKX"
      },
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(tweet_pad,df.sentiment.values,test_size=0.2)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBojBPN7e12B",
        "outputId": "4ba1405c-25d9-4371-e086-89cfe41ca461",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "train_model(X_train,y_train,embedding_matrix[0],nepochs=40,batch_size=32)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/40   -   loss: 0.56591   -   val_loss: 2.98936\n",
            "saving model...\n",
            "Epoch 2/40   -   loss: 0.52452   -   val_loss: 2.79747\n",
            "saving model...\n",
            "Epoch 3/40   -   loss: 0.50404   -   val_loss: 2.66345\n",
            "saving model...\n",
            "Epoch 4/40   -   loss: 0.48239   -   val_loss: 2.52628\n",
            "saving model...\n",
            "Epoch 5/40   -   loss: 0.45448   -   val_loss: 2.26931\n",
            "saving model...\n",
            "Epoch 6/40   -   loss: 0.42000   -   val_loss: 2.00590\n",
            "saving model...\n",
            "Epoch 7/40   -   loss: 0.38325   -   val_loss: 1.72894\n",
            "saving model...\n",
            "Epoch 8/40   -   loss: 0.34362   -   val_loss: 1.46533\n",
            "saving model...\n",
            "Epoch 9/40   -   loss: 0.30995   -   val_loss: 1.25364\n",
            "saving model...\n",
            "Epoch 10/40   -   loss: 0.28222   -   val_loss: 1.12216\n",
            "saving model...\n",
            "Epoch 11/40   -   loss: 0.25851   -   val_loss: 1.08625\n",
            "saving model...\n",
            "Epoch 12/40   -   loss: 0.24352   -   val_loss: 0.94372\n",
            "saving model...\n",
            "Epoch 13/40   -   loss: 0.22760   -   val_loss: 0.87189\n",
            "saving model...\n",
            "Epoch 14/40   -   loss: 0.22058   -   val_loss: 0.83972\n",
            "saving model...\n",
            "Epoch 15/40   -   loss: 0.21150   -   val_loss: 0.79222\n",
            "saving model...\n",
            "Epoch 16/40   -   loss: 0.20325   -   val_loss: 0.78106\n",
            "saving model...\n",
            "Epoch 17/40   -   loss: 0.20062   -   val_loss: 0.72794\n",
            "saving model...\n",
            "Epoch 18/40   -   loss: 0.19333   -   val_loss: 0.72293\n",
            "saving model...\n",
            "Epoch 19/40   -   loss: 0.19052   -   val_loss: 0.69853\n",
            "saving model...\n",
            "Epoch 20/40   -   loss: 0.18744   -   val_loss: 0.69330\n",
            "saving model...\n",
            "Epoch 21/40   -   loss: 0.18518   -   val_loss: 0.69656\n",
            "Epoch 22/40   -   loss: 0.18222   -   val_loss: 0.65549\n",
            "saving model...\n",
            "Epoch 23/40   -   loss: 0.17896   -   val_loss: 0.64822\n",
            "saving model...\n",
            "Epoch 24/40   -   loss: 0.17504   -   val_loss: 0.65038\n",
            "Epoch 25/40   -   loss: 0.17302   -   val_loss: 0.62671\n",
            "saving model...\n",
            "Epoch 26/40   -   loss: 0.17235   -   val_loss: 0.62826\n",
            "Epoch 27/40   -   loss: 0.16853   -   val_loss: 0.60897\n",
            "saving model...\n",
            "Epoch 28/40   -   loss: 0.16959   -   val_loss: 0.61749\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj3DIKRagzRl"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifu0zo84VFnx"
      },
      "source": [
        "DEVICE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCM2gl8Xe8cx"
      },
      "source": [
        "dataloader = DataLoader(TweetDataset(X_test,mode=\"test\"),batch_size=32,shuffle=False)\n",
        "model = TweetModel(embedding_matrix=embedding_matrix[0]).to(DEVICE)\n",
        "model.load_state_dict(torch.load(\"classifier.pt\"))\n",
        "model.eval()\n",
        "batch_preds = []\n",
        "predictions=[]\n",
        "for x,y in dataloader:\n",
        "  x= x.to(DEVICE)\n",
        "  with torch.no_grad():\n",
        "    batch_preds = model(x)\n",
        "\n",
        "  predictions.append(batch_preds)\n",
        "\n",
        "predictions = torch.cat(predictions, dim=0).cpu().numpy()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIW7DXXSgo8J"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsU_zNNCV_VJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkaL4X75gwst"
      },
      "source": [
        "print(roc_auc_score(np.round(predictions).squeeze(),y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlgm5rEhCGOt"
      },
      "source": [
        "## Best way to understand LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkZKkWnLJHCh"
      },
      "source": [
        "x = torch.tensor([[1,2, 12,34, 56,78, 90,80],\n",
        "                 [12,45, 99,67, 6,23, 77,82],\n",
        "                 [3,24, 6,99, 12,56, 21,22]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRklNOqVsmSH"
      },
      "source": [
        " \n",
        " emb = nn.Embedding(100,100)\n",
        " model = nn.LSTM(100,hidden_size=5,num_layers=1, batch_first=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXtdhX450moi"
      },
      "source": [
        "out1 = emb(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f228rK1gWZa_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOxgpQs7Naud"
      },
      "source": [
        "out1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQRlSttyLxof"
      },
      "source": [
        "out2,(h1,c1) = model(out1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SseXOpY3nN9z"
      },
      "source": [
        "out2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMxdxVibL2RO"
      },
      "source": [
        "mean = torch.mean(out2,1)\n",
        "max,_ = torch.max(out2,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWgeNEF6pqeT"
      },
      "source": [
        "out4 = torch.cat((mean,max),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY3fyIyIpawi"
      },
      "source": [
        "torch.nn.functional.sigmoid(nn.Linear(10,1)(out4))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0zNkjecmF0f"
      },
      "source": [
        "mean.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCh5fTSWWrxN"
      },
      "source": [
        "out2 = torch.cat((mean,max),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7hJT0HTdbAq"
      },
      "source": [
        "out2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43kzyzo-WagL"
      },
      "source": [
        "nn.Linear(16,1)(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcfxVqbkMz2b"
      },
      "source": [
        "h1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtgMW8hwM6-Z"
      },
      "source": [
        "c1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbHWTdWjNFyT"
      },
      "source": [
        "torch.max"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKuj9C9QXT_-"
      },
      "source": [
        "import logging"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJVz5R8NEkk4"
      },
      "source": [
        "logging.error(\"sdsa\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucx0q5DaEoR7"
      },
      "source": [
        "from torch.utils.model_zoo import load_url"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aNGP6DIwI34"
      },
      "source": [
        "model= load_url(url=\"https://drive.google.com/file/d/1rAvsmDZo6l1l4fpZPYKAM1D8ur3wFrYR/view?usp=sharing\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spryBX_vwVHk"
      },
      "source": [
        "torch.hub.load_state_dict_from_url('https://drive.google.com/file/d/1rAvsmDZo6l1l4fpZPYKAM1D8ur3wFrYR/view?usp=sharing')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFPKtb7HxwzC"
      },
      "source": [
        "!kaggle datasets download -d mlg-ulb/creditcardfraud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TchTcdVIFYix"
      },
      "source": [
        "import pandas as pd\n",
        "df= pd.read_csv(\"/content/creditcardfraud.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp1b64S4HyQg"
      },
      "source": [
        "df = pd.concat([df.query(\"Class==1\"),df.query(\"Class==0\").sample(7000)]).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irs6ZazFWqCN"
      },
      "source": [
        "df.to_csv(\"/content/gdrive/My Drive/credit_card.csv\",index=False )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8HZE4ICH1pf"
      },
      "source": [
        "df.Class.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wd4KKAwIM96"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}